{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to questionnaire.csv\n",
    "# Will also be the target directory for writing processed data files to\n",
    "# data_csv = Path(\"F:/\") / \"Research Project\" / \"data-analysis\" / \"r-project\" / \"data\" / \"questionnaire.csv\"\n",
    "data_csv = Path().resolve() / \"r-project\" / \"data\" / \"questionnaire.csv\"\n",
    "\n",
    "df = pd.read_csv(data_csv)\n",
    "\n",
    "df.dropna(how='all', axis=1, inplace=True) \n",
    "\n",
    "# Insert sample ids and make it the index\n",
    "df_1 = pd.DataFrame([x for x in range(0, len(df))], columns=[\"sample_id\"])\n",
    "df = df_1.join(df)\n",
    "df = df.set_index(\"sample_id\")\n",
    "\n",
    "id_database = {\"next_free_id\": 0}\n",
    "\n",
    "def person_id_map(row):\n",
    "    uid = f\"{row[\"dyad_id\"]}.{row[\"person_id\"]}\"\n",
    "    if uid not in id_database:\n",
    "        id_database[uid] = id_database[\"next_free_id\"]\n",
    "        id_database[\"next_free_id\"] += 1\n",
    "    \n",
    "    return id_database[uid]        \n",
    "\n",
    "def treatment_map(row):\n",
    "    mapping = {\n",
    "        \"A\": \"Static Face\",\n",
    "        \"B\": \"Eye Tracked\",\n",
    "        \"C\": \"Full Tracked\",\n",
    "    }\n",
    "    return mapping[row[\"treatment\"]]\n",
    "\n",
    "# Give every person a unique id\n",
    "df[\"person_id\"] = df.apply(person_id_map, axis=1)\n",
    "# Insert condition names next to condition ids\n",
    "df.insert(int(df.columns.to_list().index(\"treatment\")) + 1, \"treatment_name\", df.apply(treatment_map, axis=1))\n",
    "\n",
    "questions = {\n",
    "    \"physical_presence\": [f\"q{i}\" for i in range(1,6)],\n",
    "    \"social_presence\": [f\"q{i}\" for i in range(6,13)],\n",
    "    \"self_presence\": [f\"q{i}\" for i in range(13,18)]\n",
    "}\n",
    "\n",
    "# Line removing q11 from condideration, not in final analsyis but part of short analysis in discussions\n",
    "# q11 is the only social presence question that has a decreasing mean with higher facial realism (though not significantly decreasing)\n",
    "# questions[\"social_presence\"] = list(filter(lambda x: x not in [\"q11\"], questions[\"social_presence\"]))\n",
    "\n",
    "def calculate_physical_presence_score(df: pd.DataFrame):\n",
    "    means = df[questions[\"physical_presence\"]].mean(axis=1)\n",
    "    # sum = df[questions[\"physical_presence\"]].sum(axis=1)\n",
    "    # df = df.join(sum.rename(\"physical_presence_score\"))\n",
    "    df = df.join(means.rename(\"physical_presence_score\"))\n",
    "    return df\n",
    "\n",
    "def calculate_social_presence_score(df: pd.DataFrame):\n",
    "    means = df[questions[\"social_presence\"]].mean(axis=1)\n",
    "    # sum = df[questions[\"social_presence\"]].sum(axis=1)\n",
    "    # df = df.join(sum.rename(\"social_presence_score\"))\n",
    "    df = df.join(means.rename(\"social_presence_score\"))\n",
    "    return df\n",
    "\n",
    "def calculate_self_presence_score(df: pd.DataFrame):\n",
    "    means = df[questions[\"self_presence\"]].mean(axis=1)\n",
    "    # sum = df[questions[\"self_presence\"]].sum(axis=1)\n",
    "    # df = df.join(sum.rename(\"self_presence_score\"))\n",
    "    df = df.join(means.rename(\"self_presence_score\"))\n",
    "    return df\n",
    "\n",
    "def calculate_presence_score(df: pd.DataFrame):\n",
    "    means = df[[f\"q{i}\" for i in range(1,18)]].mean(axis=1)\n",
    "    # sum = df[questions[\"self_presence\"]].sum(axis=1)\n",
    "    # df = df.join(sum.rename(\"self_presence_score\"))\n",
    "    df = df.join(means.rename(\"presence_score\"))\n",
    "    return df\n",
    "\n",
    "def calculate_scores(df: pd.DataFrame):\n",
    "    # df = df[[f\"q{i}\" for i in range(1,18)]] - 1\n",
    "    df = calculate_physical_presence_score(df)\n",
    "    df = calculate_social_presence_score(df)\n",
    "    df = calculate_self_presence_score(df)\n",
    "    return calculate_presence_score(df)\n",
    "\n",
    "df = calculate_scores(df)\n",
    "\n",
    "for (treatment, treatment_df) in df.groupby(\"treatment\"):\n",
    "    physical_presence = treatment_df[\"physical_presence_score\"].to_numpy()\n",
    "    social_presence = treatment_df[\"social_presence_score\"].to_numpy()\n",
    "    self_presence = treatment_df[\"self_presence_score\"].to_numpy()\n",
    "    \n",
    "    physical_summary = {\"mean\": np.mean(physical_presence), \"std\": np.std(physical_presence)}\n",
    "    social_summary = {\"mean\": np.mean(social_presence), \"std\": np.std(social_presence)}\n",
    "    self_summary = {\"mean\": np.mean(self_presence), \"std\": np.std(self_presence)}\n",
    "    \n",
    "    print(f\"=== Scenario: {treatment} ===\")\n",
    "    print(f\"Physical: mean score = {physical_summary['mean']:.5f}, std = {physical_summary['std']:.5f}\")\n",
    "    print(f\"Social  : mean score = {social_summary['mean']:.5f}, std = {social_summary['std']:.5f}\")\n",
    "    print(f\"Self    : mean score = {self_summary['mean']:.5f}, std = {self_summary['std']:.5f}\")\n",
    "    print()\n",
    "\n",
    "# print(questions[\"social_presence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full dataframe with calculated scores\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop question answers from data, leaving only calculated subscale scores\n",
    "# data = data.drop(questions[\"physical_presence\"], axis=1, errors=\"ignore\")\n",
    "# data = data.drop(questions[\"social_presence\"], axis=1, errors=\"ignore\")\n",
    "# data = data.drop(questions[\"self_presence\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(df)\n",
    "\n",
    "df.to_csv(data_csv.parent / \"data_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process recorded experiment times data\n",
    "from dateutil import parser\n",
    "\n",
    "df_times = pd.read_csv(data_csv.parent / \"experiment_times.csv\")\n",
    "df_times = df_times.set_index(\"sample_id\")\n",
    "# display(df_times)\n",
    "\n",
    "for index, row in df_times.iterrows():\n",
    "    time_1: str = row[\"time_formatted_1\"]\n",
    "    time_2: str = row[\"time_formatted_2\"]\n",
    "    time_1_minutes, time_1_seconds = map(np.double, time_1.split(\":\"))\n",
    "    time_2_minutes, time_2_seconds = map(np.double, time_2.split(\":\"))\n",
    "    total_time_seconds = (time_1_minutes + time_2_minutes) * 60 + time_1_seconds + time_2_seconds\n",
    "    \n",
    "    df_times.at[index, \"time_seconds_total\"] = total_time_seconds\n",
    "    \n",
    "for (treatment, treatment_df) in df_times.groupby(\"treatment\"):\n",
    "    time_data_mins = treatment_df[\"time_seconds_total\"].to_numpy() / 60\n",
    "    print(f\"=== Scenario: {treatment} ===\")\n",
    "    print(f\"mean time = {np.mean(time_data_mins):.5f}, std = {np.std(time_data_mins):.5f}\")\n",
    "\n",
    "df_times.to_csv(data_csv.parent / \"experiment_times.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "\n",
    "\n",
    "\n",
    "# This cell is for generating a hypothetical case where the sample size was 4 times bigger (28 dyads instead of 7)\n",
    "# This generated data was obviously not used in primary data analysis,\n",
    "# only for testing if a bigger sample size would improve significance with this data.\n",
    "\n",
    "# Data was copied, dyad/person ids changed and data randomly fuzzed. New data was then appended to original data.\n",
    "# This happens 3 times for a 4x sample size dataset\n",
    "\n",
    "# display(df)\n",
    "\n",
    "# df_1 = df.copy()\n",
    "# dyads = df_1[\"dyad_id\"].unique()\n",
    "# print(dyads)\n",
    "\n",
    "# # Shift new ids up to make room for generating new ones by adding 1 every time\n",
    "# df_1.dyad_id *= 10\n",
    "\n",
    "# for i in range(3):\n",
    "#     df_1.dyad_id += 1\n",
    "#     df_1.person_id += 13 + 1\n",
    "    \n",
    "#     df_1.social_presence_score = df.social_presence_score\n",
    "#     df_1.physical_presence_score = df.physical_presence_score\n",
    "#     df_1.self_presence_score = df.self_presence_score\n",
    "    \n",
    "#     score_diff = (np.random.random(len(df_1)) - 0.5) * (0.4)\n",
    "    \n",
    "#     df_1.social_presence_score += score_diff\n",
    "#     df_1.physical_presence_score += score_diff\n",
    "#     df_1.self_presence_score += score_diff\n",
    "#     df_1.index = df_1.index + len(df_1)\n",
    "\n",
    "#     df = pd.concat([df, df_1])\n",
    "\n",
    "# df.to_csv(data_csv.parent / \"data_calculated_x4.csv\")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo sampling to generate dataset with higher sample size.\n",
    "# Randomly selected dyads have a randomly distributed noise added (with a standard distribution matching the relevant score and treatment)\n",
    "# This code is likely horribly inefficient and was written in a time crunch as a more correct replacement of the other data generation code above\n",
    "# This cell takes more than 1 hour to run.\n",
    "\n",
    "\n",
    "dyads = df[\"dyad_id\"].unique()\n",
    "treatments = df[\"treatment\"].unique()\n",
    "scales = [\"physical_presence_score\", \"social_presence_score\", \"self_presence_score\"]\n",
    "print(dyads, treatments)\n",
    "\n",
    "std = {}\n",
    "\n",
    "for treatment in treatments:\n",
    "    for scale in scales:\n",
    "        # display(df[df[\"treatment\"] == treatment][scale])\n",
    "        # std[treatment][scale] = np.std(df[df[\"treatment\"] == treatment, scale], ddof=1)\n",
    "        if treatment not in std:\n",
    "            std[treatment] = {}\n",
    "        std[treatment][scale] = np.std(df[df[\"treatment\"] == treatment][scale])\n",
    "        \n",
    "        \n",
    "times = 10\n",
    "sample_size = len(dyads) * times\n",
    "\n",
    "max_simulations = 100\n",
    "\n",
    "for simulation in range(max_simulations):\n",
    "    break\n",
    "    next_dyad_id = 0\n",
    "    generated_data = pd.DataFrame()\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        selected_dyad_id = np.random.choice(dyads)\n",
    "        selected_dyad = df[df.dyad_id == selected_dyad_id]\n",
    "        # print(selected_dyad_id)\n",
    "        \n",
    "        new_dyad = selected_dyad.copy()\n",
    "        new_dyad.dyad_id = next_dyad_id\n",
    "        next_dyad_id += 1\n",
    "        # display(new_dyad)\n",
    "        \n",
    "        for treatment in treatments:\n",
    "            for scale in scales:\n",
    "                # display(new_dyad)\n",
    "                noise = np.zeros((2,))\n",
    "                for i in range(2):\n",
    "                    noise[i] = np.random.normal(0, std[treatment][scale])\n",
    "                    \n",
    "                column = new_dyad.loc[new_dyad.treatment == treatment, scale]\n",
    "                \n",
    "                column += noise\n",
    "                \n",
    "                # Clamp column to range [1, 7]\n",
    "                column = column.clip(lower = 1, upper = 7)\n",
    "                \n",
    "                # Update column to new column with normally distributed noise\n",
    "                new_dyad.loc[new_dyad.treatment == treatment, scale] = column\n",
    "        \n",
    "        if generated_data.empty:\n",
    "            generated_data = new_dyad\n",
    "        else:\n",
    "            generated_data = pd.concat([generated_data, new_dyad])\n",
    "\n",
    "        # Fix person_ids, give every person in \n",
    "        id_database = {\"next_free_id\": 0}\n",
    "        generated_data[\"person_id\"] = generated_data.apply(person_id_map, axis=1)\n",
    "\n",
    "        # with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "            # display(generated_data)\n",
    "\n",
    "        out_file = data_csv.parent / \"generated\" / f\"generated_data_simulation_{simulation}.csv\"\n",
    "        if not out_file.parent.exists:\n",
    "            out_file.parent.mkdir()    \n",
    "        generated_data.to_csv(out_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
